--- 
title: "Want population level inferences? Use sum-to-zero contrasts"
author: "Andrew J. Tyre & Trevor Hefley"
date: "`r format( Sys.Date(), '%B %d, %Y' )`"
output:
  bookdown::html_document2:
    theme: lumen
    highlight: tango
    number_sections: FALSE
    toc: TRUE
    toc_float: TRUE
    code_folding: hide
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: TRUE
github-repo: atyre2/population-inference
description: "This is a paper about using sum to zero contrasts to get population level inference."
---

# Introduction {#intro}

A sum-to-zero contrast codes a categorical variable as deviations from a grand mean. Social scientists use them extensively. Should ecologists?[^CVpost]

More accurately, should I tell my students in Ecological Statistics to use them? Sum-to-zero contrasts are conceptually similar to centering a continuous variable by subtracting the mean from your predictor variable prior to analysis. Discussions of centering often end up conflated with *scaling*, which is dividing your predictor variable by a constant, usually the standard deviation, prior to analysis.  *Always scaling* covariates prior to regression analysis is controversial advice. See for example [Andrew Gelman's blogpost and comments](http://andrewgelman.com/2009/07/11/when_to_standar/), or many crossvalidated questions [such as this one](http://stats.stackexchange.com/q/29781/12258) which has links to many others. There is a good reference as well as some useful discussion in the comments of [this question](http://stats.stackexchange.com/questions/179732/motivation-to-center-continuous-predictor-in-multiple-regression-for-sake-of-mul). In this post I focus on the effects of sum to zero contrasts for categorical variables and interactions with continuous variables.[^allthecode]

```{r setup, echo=FALSE, include=FALSE, message=FALSE}
# load necessary packages here
knitr::opts_chunk$set(tidy=TRUE)
library(ggplot2)
library(dplyr) #Stay in the tidyverse! 
library(broom)
#library(sjPlot)
library(stargazer)
library(pander)
#library(memisc)
```

Here is my summary of the pros and cons of centering drawn from those references above.

* Centering continuous variables eliminates collinearity between 
    interaction and polynomial terms and the individual covariates 
    that make them up.
* Centering does not affect inference about the covariates.
* Centering can improve the interpretability of the coefficients in
    a regression model, particularly because the intercept
    represents the value of the response at the mean of the 
    predictor variables.
* Predicting out of sample data with a model fitted to centered 
    data must be done carefully because the center of the out of
    sample data will be different from the fitted data.
* There may be some constant value other than the sample mean
    that makes more sense based on domain knowledge.

# A simple example {#simple}

To make the discussion concrete, let me demonstrate with an example of the interaction between a continuous covariate and a categorical one. In the following I refer to the effect of individual covariates outside the interaction as the "simple effect" of that covariate.

```{r , warning=FALSE, results = "asis"}
data(iris)
# flip the treatment contrast so coefficient labels match
contrasts(iris$Species) <- contr.SAS(3)
m0 <- lm(Sepal.Width~Sepal.Length*Species,data=iris)
summary_m0 <- summary(m0)
iris_centered <- iris %>% mutate(Sepal.Length=Sepal.Length - mean(Sepal.Length))
m1 <- lm(Sepal.Width~Sepal.Length*Species,data=iris_centered)
summary_m1 <- summary(m1)
iris_csz <- iris_centered
contrasts(iris_csz$Species) <- contr.sum(3)
m2 <- lm(Sepal.Width~Sepal.Length*Species,data=iris_csz)
summary_m2 <- summary(m2)
m3 <- lm(Sepal.Width~Sepal.Length,data=iris)
summary_m3 <- summary(m3)
m4 <- lm(Sepal.Width~Sepal.Length + Species, data = iris_csz)
summary_m4 <- summary(m4)


stargazer(m0, m1, m2, m3, m4,
          title="Linear regressions of Sepal Width on species and Sepal Length for different parameterizations. The default treatment contrast has been changed to match the SAS default so that covariate labels match between the treatment and sum-to-zero contrasts.",
          column.labels = c("Interaction", "Interaction and Centered", "Interaction, centered and sum-to-zero", "Length only", "Length and Species"),
          intercept.bottom = FALSE, intercept.top = TRUE,
          keep.stat = c("adj.rsq", "ser"),
          align=TRUE,
          type = "html",
          style = "ajs" # "ajs"
)
#           summary.stats = FALSE,
#            coef.style = "all.nostar")
#write.mtable(mtable123, format = "LaTeX" )
#sjt.lm(m0,m1)# You have to save the table in html format.
#pander(mtable123)
#htmltools::includeHTML("output.html")
#knitr::kable(mtable123)
```

The intercept of this model isn't directly interpretable because it gives the average width at a length of zero, which is impossible. In addition, both the intercept and simple effect of length represent the change in width for only one species, *virginica*. The treatment contrast in R estimates coefficients for $k - 1$ levels of a factor. In the simple effect of a factor each coefficient is the difference between the last level (estimated by the intercept), and the named level. In the above, `Species1` has sepals that are 2 mm narrower than *virginica* at a length of zero. The interaction coefficients
such as `Sepal.Length:Species1` tell us how much the effect of Sepal.Length in *versicolor* changes from that in *setosa*. So every mm of sepal length in versicolor increases sepal width by $0.8 - 0.48 = 0.32$ mm. 

```{r basePlot, fig.cap="Sepal Length against Sepal Width for three species of iris."}

base_iris <- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) + 
  geom_point(aes(shape = Species)) + 
  xlab("Sepal Length [mm]") + 
  ylab("Sepal Width [mm]")

nd <- expand.grid(Sepal.Length = seq(-1, 8, 0.1),
                  Species = factor(levels(iris$Species)))
pred.0 <- augment(m0, newdata = nd)             
base_iris + geom_line(aes(y = .fitted, linetype = Species), data = pred.0)
```

I did something rather silly in Figure \@ref(fig:basePlot) because I wanted to see where the curves cross x = 0. That is where the estimates for the intercept and simple effect of species are calculated. The intercept is negative, and the line for *setosa* crosses x = 0 well below y = 0. The simple effect estimates of Species are both positive, with *virginica* being larger, and indeed the line for *virginica* crosses x = 0 at the highest point. Similarly, the simple effect of length is the slope of the line for *setosa*, and it is larger than the slopes of the other two species because the estimated interactions are both negative. But not centering really makes things ugly for direct interpretation of the estimated coefficients. 

Now we repeat the analysis after centering sepal.length. Although the coefficients change because now the model estimates the differences between the species at the mean length, the t-statistics for the continuous covariate, including the interaction terms, do not change. The t-statistics for the intercept and simple effect of species do change. That's OK, because they are really testing something quite different from before.  

What happens if we use sum to zero contrasts for species? I can now directly interpret the intercept as the average width at the average length, averaged over species. Similarly the simple effect of length as the change in width averaged across species. This seems like a very useful set of coefficients to look at, particularly if my categorical covariate has many levels. 
You might ask (I did), what do the categorical coefficients mean? They represent deviations from the mean intercept for each species. OK, you say (I did), then where's species *virginica*? After much head scratching, the answer is that it is the negative of the **sum** of the other two coefficients. 

I just thought of something else. Are those "average" intercept and slope the same as what I would get if I only use `cSepal.Length`?
Whoa! It is not the same, in fact it is radically different. Totally different conclusion about the "average" effect. 
This is closer to the conclusion obtained with the interaction model. 

I have seen assertions in some papers (particularly from social sciences), that using sum to zero contrasts (also called effects coding, I believe), allows the direct interpretation of lower order terms in an ANOVA table even in the presence of interactions. 

```{r}
pander(anova(m2)) # doesn't matter which model I use
```

If so, in this case I could say "Sepal Width differs significantly between species." I'm not sure I believe that. The ANOVA table is identical between all 3 models, whether I use sum-to-zero contrasts or not. Why should the interpretation differ if I just change the parameterization of the model? 

Explaining treatment contrasts to students is a pain. I'm not sure that these are any easier. I have a few thoughts about the effects of sum-to-zero contrasts and model averaging, but that will have to wait for a different post. 

[^allthecode]: All the code for this post, including that not shown, [can be found here](https://github.com/atyre2/atyre2.github.io/raw/master/_drafts/sum-to-zero-contrasts.Rmd).

[^CVpost]: This stuff first appeared [as a question on CrossValidated](http://stats.stackexchange.com/questions/188852/centering-in-the-presence-of-interactions-with-categorical-predictors), but received no attention. Then AJT made it into [a blog post](http://atyre2.github.io/2016/09/03/sum-to-zero-contrasts.html), where it still received no attention! Upping the ante here ...
