--- 
title: "Want population level inferences? Use sum-to-zero contrasts"
author: "Andrew J. Tyre & Trevor Hefley"
date: "`r format( Sys.Date(), '%B %d, %Y' )`"
output:
  bookdown::html_document2:
    theme: lumen
    highlight: tango
    number_sections: FALSE
    toc: TRUE
    toc_float: TRUE
    code_folding: hide
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: TRUE
github-repo: atyre2/population-inference
description: "This is a paper about using sum to zero contrasts to get population level inference."
---

# Introduction {#intro}

A sum-to-zero contrast codes a categorical variable as deviations from a grand mean. Social scientists use them extensively. Should ecologists?[^CVpost]

More accurately, should I tell my students in Ecological Statistics to use them? Sum-to-zero contrasts are conceptually similar to centering a continuous variable by subtracting the mean from your predictor variable prior to analysis. Discussions of centering often end up conflated with *scaling*, which is dividing your predictor variable by a constant, usually the standard deviation, prior to analysis.  *Always scaling* covariates prior to regression analysis is controversial advice. See for example [Andrew Gelman's blogpost and comments](http://andrewgelman.com/2009/07/11/when_to_standar/), or many crossvalidated questions [such as this one](http://stats.stackexchange.com/q/29781/12258) which has links to many others. There is a good reference as well as some useful discussion in the comments of [this question](http://stats.stackexchange.com/questions/179732/motivation-to-center-continuous-predictor-in-multiple-regression-for-sake-of-mul). In this post I focus on the effects of sum to zero contrasts for categorical variables and interactions with continuous variables.[^allthecode]

```{r setup, echo=FALSE, include=FALSE, message=FALSE}
# load necessary packages here
knitr::opts_chunk$set(tidy=TRUE)
library(ggplot2)
library(dplyr) #Stay in the tidyverse! 
library(broom)
#library(sjPlot)
library(stargazer)
library(pander)
#library(memisc)
```

Here is my summary of the pros and cons of centering drawn from those references above.

* Centering continuous variables eliminates collinearity between 
    interaction and polynomial terms and the individual covariates 
    that make them up.
* Centering does not affect inference about the covariates.
* Centering can improve the interpretability of the coefficients in
    a regression model, particularly because the intercept
    represents the value of the response at the mean of the 
    predictor variables.
* Predicting out of sample data with a model fitted to centered 
    data must be done carefully because the center of the out of
    sample data will be different from the fitted data.
* There may be some constant value other than the sample mean
    that makes more sense based on domain knowledge.

# A simple example {#simple}

To make the discussion concrete, let me demonstrate with an example of the interaction between a continuous covariate and a categorical one. In the following I refer to the effect of individual covariates outside the interaction as the "simple effect" of that covariate.

```{r , warning=FALSE, results = "hide"}
data(iris)
# flip the treatment contrast so coefficient labels match
new_contrast <- contr.SAS(3)
colnames(new_contrast) <- c("setosa", "versicolor")
contrasts(iris$Species) <- new_contrast

m0 <- lm(Sepal.Width~Sepal.Length*Species,data=iris)
summary_m0 <- summary(m0)
iris_centered <- iris %>% mutate(Sepal.Length=Sepal.Length - mean(Sepal.Length))
m1 <- lm(Sepal.Width~Sepal.Length*Species,data=iris_centered)
summary_m1 <- summary(m1)
iris_csz <- iris_centered
sum_contrast <- contr.sum(3)
colnames(sum_contrast) <- colnames(new_contrast)
contrasts(iris_csz$Species) <- sum_contrast
m2 <- lm(Sepal.Width~Sepal.Length*Species,data=iris_csz)
summary_m2 <- summary(m2)
m3 <- lm(Sepal.Width~Sepal.Length,data=iris_csz)
summary_m3 <- summary(m3)
m4 <- lm(Sepal.Width~Sepal.Length + Species, data = iris_csz)
summary_m4 <- summary(m4)


tableout <- stargazer(m0, m1, m2, m3, m4,
          type = "html",
          column.labels = c("Interaction", "Interaction and Centered", "Interaction, centered and sum-to-zero", "Length only", "Length and Species"),
          intercept.bottom = FALSE, intercept.top = TRUE,
          keep.stat = c("adj.rsq", "ser"),
          align=TRUE,
          style = "ajs" # "ajs"
)

```

```{r regTable, echo=FALSE}
knitr::kable(tableout, caption = "Linear regressions of Sepal Width on species and Sepal Length for different parameterizations. The default treatment contrast has been changed to match the SAS default so that covariate labels match between the treatment and sum-to-zero contrasts.")
```


The intercept of the interaction model isn't directly interpretable because it gives the average width at a length of zero, which is impossible. In addition, both the intercept and simple effect of length represent the change in width for only one species, *virginica*. The treatment contrast in R estimates coefficients for $k - 1$ levels of a factor. In the simple effect of a factor each coefficient is the difference between the last level (estimated by the intercept), and the named level. In the above, `Speciessetosa` has sepals that are 2 mm narrower than *virginica* at a length of zero. The interaction coefficients
such as `Sepal.Length:Speciessetosa` tell us how much the effect of Sepal.Length in *setosa* changes from that in *virginica*. So every mm of sepal length in *setosa* increases sepal width by $.23 + .57 = 0.8$ mm. 

```{r basePlot, fig.cap="Sepal Length against Sepal Width for three species of iris."}

base_iris <- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) + 
  geom_point(aes(shape = Species)) + 
  xlab("Sepal Length [mm]") + 
  ylab("Sepal Width [mm]")

nd <- expand.grid(Sepal.Length = seq(-1, 8, 0.1),
                  Species = factor(levels(iris$Species)))
pred.0 <- augment(m0, newdata = nd)             
base_iris + geom_line(aes(y = .fitted, linetype = Species), data = pred.0) + geom_vline(xintercept = mean(iris$Sepal.Length), color = "red", linetype  = 4)
```

I did something rather silly in Figure \@ref(fig:basePlot) because I wanted to see where the curves cross x = 0. That is where the estimates for the intercept and simple effect of species are calculated. The intercept is positive, and the line for *virginica* crosses x = 0 well above y = 0. The simple effect estimates of Species are both negative, with *setosa* being larger, and indeed the line for *setosa* crosses x = 0 at the lowest point. Similarly, the simple effect of length is the slope of the line for *virginica*, and it is smaller than the slopes of the other two species because the estimated interactions are both positive. But not centering really makes things ugly for direct interpretation of the estimated coefficients. 

Now we repeat the analysis after centering `sepal.length`. The estimates and the standard errors for the continuous covariate, including the interaction terms, do not change. The slopes of the three lines are not affected by centering the continuous variable. The estimates and standard errors for the intercept and simple effect of species do change. That's OK, because they are really testing something quite different from before. Now these coefficients are telling us what happens at the mean sepal length. Looking back at Figure \@ref(fig:basePlot), the mean is a bit less than 6 mm, *versicolor* is about the same size as *virginica*, while *setosa* is quite a bit bigger. And that's what the simple effects coefficients tell us in the centered model (2nd column of Table \@ref(tab:regTable)). The coefficient for *setosa* is large (relative to SE) and positive, while the coefficient for *versicolor* is small and negative.

What happens if we use sum to zero contrasts for species? I can now directly interpret the intercept as the average width at the average length, averaged over species. Similarly the simple effect of length as the change in width averaged across species. This seems like a very useful set of coefficients to look at, particularly if my categorical covariate has many levels. 
You might ask (I did), what do the categorical coefficients mean? They represent deviations from the mean intercept for each species. OK, you say (I did), then where's species *virginica*? After much head scratching, the answer is that it is the negative of the **sum** of the other two coefficients. 

I just thought of something else. Are those "average" intercept and slope the same as what I would get if I only use `Sepal.Length`?
Whoa! It is not the same, in fact it is radically different. Totally different conclusion about the "average" effect. So, an "average" while admitting that species differ is very different from assuming there is no difference between species. That makes sense, I guess. 

## Interpreting non-marginal effects

I have seen assertions in some papers (particularly from social sciences), that using sum to zero contrasts (also called effects coding, I believe), allows the direct interpretation of lower order terms in an ANOVA table even in the presence of interactions. 

```{r}
pander(anova(m2)) # doesn't matter which model I use
```

If so, in this case I could say "Sepal Width differs significantly between species." I'm not sure I believe that. The ANOVA table is identical between all 3 models, whether I use sum-to-zero contrasts or not. Why should the interpretation differ if I just change the parameterization of the model? 

# Population level inference: sum to zero vs. random effect. 

Deguines et al. [@Deguinesetal2017] published an interesting dataset of abundances and densities of different species on multiple plots in multiple years. The key question was how does density of each species respond to both precipitation and the presence or abundance of other species. Here I will focus on just one species and compare two different ways of getting at the precipitation response in an "average" plot.

```{r example2setup, message=FALSE, fig.cap="Kangaroo rat density as a function of precipitation for 30 plots in 7 years."} 
deguines <- read.csv("http://datadryad.org/bitstream/handle/10255/dryad.131841/Deguines_etal_JAE_carrizo.csv?sequence=1")
ggplot(deguines, mapping = aes(x = Precip, y = Krats, color = plot_id)) + geom_point() + 
  geom_line() + guides(color=FALSE) + xlab("Precipitation [mm]") + ylab("Kangaroo Rat Density")
```

Deguines et al. did a thorough job of evaluating the assumptions of the model estimated using `lme()`, so we will simply start with their top model, which had an interaction between precipitation and plant biomass, a random effect of plot identity, and allowed the variance to change between years. The only thing I will change is that I will center the two continuous variables to ease the interpretation of the population average effects. 

```{r mixedModel}
library(nlme)

# centering
deguines_c <- deguines %>% 
  mutate(Precip = Precip - mean(Precip),
         Plant_biomass= Plant_biomass - mean(Plant_biomass))

# Specifying control values for lme fit:
lmecontr_foc <- lmeControl(maxIter=50, msMaxIter=50) #the default

mixedmod <- lme(Krats ~ Precip*Plant_biomass,
                weights = varIdent(form = ~ 1 | fyear),
                random = ~ 1|plot_id,
                data=deguines_c, method = "REML", control=lmecontr_foc)
summary(mixedmod)
```

So, there are $\approx 33$ Krats with average precipitation and plant biomass. At the average plant biomass, increasing precipitation increases density by about 1 Krat per mm. As biomass increases holding the average precipitation constant, the density of Krats goes *down*. That's interesting, because it is the opposite sign from the model estimated with the uncentered data! Regardless of centering, the effect of precipitation gets smaller (or even negative) with increasing plant biomass. Centering didn't change any of the model level inferential statistics, nor the variance estimates. It did reduce the correlations between the parameter estimates, which means the model was easier to fit. Centering seems like a universal good. 

Now I will re-fit the model treating `plot_id` as a sum-to-zero contrast fixed effect. 

```{r sz_model}
contrasts(deguines_c$plot_id) <- contr.sum(30)
szmodel <- lm(Krats ~ Precip*Plant_biomass + plot_id,
                data=deguines_c)
huxtable::huxreg(szmodel)
```

Looking at the coefficients we are seeing a very similar interpretation emerging. If we plot the random effect estimates against the plot coefficients we see there is a close correspondence (Figure \@ref(fig:ranefvsfixef)), but in general the random effects estimates are smaller. This is "shrinkage" towards zero, caused by assuming the random effects come from a normal distribution. 

```{r ranefvsfixef, fig.cap="Random effects of each plot versus the estimated fixed effect for each plot. Solid black line is the 1:1 line."}
mixedmod_effects <- tidy(mixedmod, effects="fixed")
plot_id_mm <- ranef(mixedmod)
# promote rownames to a column
plot_id_mm <- mutate(plot_id_mm, 
                     term = "(Intercept)",
                     group = "plot_id",
                     level = rownames(plot_id_mm)) %>%
  rename(estimate = `(Intercept)`)
szmodel_effects <- tidy(szmodel)
# need to add group and level to fixed_effects, and convert to estimates for all groups
# extract sz coefs into seperate data frame
plot_id_sz <- filter(szmodel_effects, startsWith(szmodel_effects$term,"plot_id"))
szmodel_effects <- filter(szmodel_effects, !startsWith(szmodel_effects$term,"plot_id"))
# add group and level columns
contrmatrix <- contrasts(deguines_c$plot_id)
# add empty row see https://stackoverflow.com/questions/41151350/append-empty-row-to-a-dataframe
plot_id_sz[nrow(plot_id_sz)+1,] <- NA
plot_id_sz <- mutate(plot_id_sz, 
                     term = "(Intercept)",
                     group = "plot_id",
                     level = rownames(contrmatrix))
# calculate estimated coef for final row 
plot_id_sz[nrow(plot_id_sz), "estimate"] <- -sum(plot_id_sz$estimate[-nrow(plot_id_sz)])
# SE involves the estimated vcov matrix ... leave that out for now.
names(plot_id_mm) <- paste0(names(plot_id_mm),".mm")
names(plot_id_sz) <- paste0(names(plot_id_sz),".sz")
plot_id_effects <- bind_cols(plot_id_mm, plot_id_sz)
ggplot(data = plot_id_effects, 
       mapping = aes(x = estimate.sz, y = estimate.mm)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0) + 
  geom_smooth(method = "lm") + 
  xlab("Sum to zero estimates") + 
  ylab("Mixed model random effects estimates")
```

There is another thing that is easy to do with the sum to zero estimates that is harder with the mixed model. We can ask if each individual plot is different from the population average! That is the p value in Table 

* what about making population average predictions from lm()?


[^allthecode]: All the code for this post, including that not shown, [can be found here](https://github.com/atyre2/population-inference/raw/master/index.Rmd).

[^CVpost]: This stuff first appeared [as a question on CrossValidated](http://stats.stackexchange.com/questions/188852/centering-in-the-presence-of-interactions-with-categorical-predictors), but received no attention. Then AJT made it into [a blog post](http://atyre2.github.io/2016/09/03/sum-to-zero-contrasts.html), where it still received no attention! Upping the ante here ...
